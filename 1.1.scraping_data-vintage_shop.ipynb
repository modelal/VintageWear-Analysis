{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99ca9da",
   "metadata": {},
   "source": [
    "# Vintage Shop - Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bbc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#BeautifulSoup es una calse dentro de la libreria bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdfb8c",
   "metadata": {},
   "source": [
    "### Scrapping Aproach:\n",
    "\n",
    "Fist, I've acces the main page and explore the configuration and structure.\n",
    "\n",
    "I will be targeting all products, filtered as t-shirts\n",
    "\n",
    "The page provides with a selection list to filter categories.I observed the same elements can be shown in more than one category. By analizing the products shown in in collection, we've notice that all items are at least in one the decades collections, and items that are in a decade selection are not in other decade, so by filtering by decade we will have each element in one specific category, with out dupplicates.\n",
    "\n",
    "We want to retrive, the description, the selling price, the collection(decade) in with it is included and the link to the product especifications.\n",
    "\n",
    "By analizing the html code I selected the main structure that shared all products, the \"Product Card\".\n",
    "\n",
    "We will target all \"Product Card\" items in the same page and then iterate trough each of them to extract the required values.\n",
    "\n",
    "Take in count that each collections has its products display in more than one page, and number of pages change between collections. We will need to know how many pages per colection there is in order to srap all products. \n",
    "\n",
    "Once we know who many pages there is in the same collection, we apply the next steps for each page:\n",
    "\n",
    "1. Extract wanted values from each product of the page.\n",
    "2. Set values to a DataFrame\n",
    "3. Concatenate DataFrame pag(n) to the all_pages data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085d2c5",
   "metadata": {},
   "source": [
    "### 0. Analizing the url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be443f8",
   "metadata": {},
   "source": [
    "By analizing the webpage and the sub-pages that we want to target.\n",
    "Determine a fromated link that allow us to enter all pages by \n",
    "changing two variables; collection and page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54dc7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"1980s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b48c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_no = 1 #works as tring and digit. we prefer digit for iteration in range (pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a284ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.wycovintage.com/collections/{collection}?filter.p.product_type=Shirt&options%5Bprefix%5D=last&page={page_no}&sort_by=created-descending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58a2d80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(url_90)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e3730c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3767a",
   "metadata": {},
   "source": [
    "### 1. Explore collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5c73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.wycovintage.com/collections/all-products\"\n",
    "html = requests.get(url)\n",
    "print(html)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaedfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colections_tab = soup.find(\"select\", {\"name\":\"collection\"})\n",
    "collections_list = [i.getText().strip().replace(\" \",\"-\").lower() for i in colections_tab.find_all(\"option\")]\n",
    "#collections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3b4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_colections = [\"select-a-collection\", \"items-under-$200\", \n",
    "    \"displays\",'all-products','backstage-passes','jackets',\n",
    "    'jerseys','new-arrivals','shirts','shorts','skate','sweaters',\n",
    "    'sweatshirts']\n",
    "\n",
    "extract_colections = [i for i in collections_list if i not in exclude_colections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd4a32",
   "metadata": {},
   "source": [
    "### 2. Acces collections by pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4979b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acces_collections(collection,page_no=1): \n",
    "    \n",
    "    \"This function generates a soup of the page 'x' of a filtered search based on collections\"\n",
    "    \n",
    "    #1. We defin the url for the search based on collection and page:\n",
    "    url = f\"https://www.wycovintage.com/collections/{collection}?filter.p.product_type=Shirt&options%5Bprefix%5D=last&page={page_no}&sort_by=created-descending\"\n",
    "\n",
    "    #2. Reques the html code for the url mentioned\n",
    "    html = requests.get(url)\n",
    "    \n",
    "    #3. Print the request resoponse, for visibility \n",
    "    print(html)\n",
    "    \n",
    "    #4. Return an objetct type BeautifulSoup with the content parsed\n",
    "    return BeautifulSoup(html.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05b3df",
   "metadata": {},
   "source": [
    "### 3. Extract nº of pages of a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "92b106c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_page(soup):\n",
    "    \n",
    "    \"This function returns the number of pages inside the same filtered shearch\"\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        #1. Find the element where pagination/index is contained\n",
    "        index = soup.find(\"ul\", {\"class\":\"Pagination\"})\n",
    "\n",
    "        #2. Extract all elements inside pagination/index that represet a number page - list returned\n",
    "        pags_list = index.find_all(\"a\", {\"class\":\"Button Pagination__Link\"})\n",
    "\n",
    "        #3. Print the last page, for visibility \n",
    "        print(f'Last page in pagination is nº{pags_list[-1].getText()}')\n",
    "\n",
    "        #3. We acces the last element of the list = last page and retunr the text\n",
    "        return int(pags_list[-1].getText())\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"There is just 1 page\")\n",
    "        return int(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444326",
   "metadata": {},
   "source": [
    "### 4. Generate DF with all values from the same page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4dc2d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_table(soup, collection):\n",
    "    \n",
    "    \"\"\"Returns a DataFrame for a given soup\"\"\"\n",
    "    \n",
    "    #1. Fist we target the box elements where all the info of a product is contained:\n",
    "    elemnt_box = soup.find_all(\"div\", {\"class\":\"ProductCard\"})\n",
    "    \n",
    "    #2. We iterate trouth the elements to extract the info that we need: Comprehension lists\n",
    "    \"\"\"This work arround is not that straigth foward but we notice a bugg (see: debugging 1)\"\"\"\n",
    "    \n",
    "    #2.1 Descriptions list:\n",
    "    descriptions = [ i.find(\"h2\", {\"class\":\"ProductCard__Title\"}).getText().strip() for i in elemnt_box]\n",
    "    \n",
    "    #2.2 Prices list:\n",
    "    prices = [ i.find(\"span\", {\"class\":\"Price\"}).getText().strip() for i in elemnt_box]\n",
    "    \n",
    "    #2.3 Links:\n",
    "    links = [ \"https://www.wycovintage.com\" + i.find(\"a\").get(\"href\") for i in elemnt_box]\n",
    "    \n",
    "    #3. Create a disctionary with the tree lists:\n",
    "    page_dict = {}\n",
    "    \n",
    "    page_dict[\"description\"] = descriptions\n",
    "    page_dict[\"price\"] = prices\n",
    "    # There is an entry for each T-shit, even if its repeated -- will use it for counting later\n",
    "    page_dict[\"qty\"] = [1 for i in range(len(descriptions))]\n",
    "    page_dict[\"collection\"] = [collection for i in range(len(descriptions))]\n",
    "    page_dict[\"link\"] = links\n",
    "    \n",
    "    df = pd.DataFrame(page_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539eabe",
   "metadata": {},
   "source": [
    "### 6. Concatenate all pages in the same collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f56bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages_collection(collection):\n",
    "    \n",
    "    \"\"\"Given a colection to target it extrats all selected values\n",
    "        for all the pages in the collection and group them in a \n",
    "        unic dataframe \"\"\"\n",
    "     \n",
    "    #1. Create an empty DataFrame (for agreggate date)\n",
    "    df_t = pd.DataFrame()\n",
    "    \n",
    "    #2. Scrap the first page of the collection\n",
    "    soup = acces_collections(collection, page_no=1)\n",
    "    \n",
    "    #3. Determine how many pages the collection have\n",
    "    pages = last_page(soup)\n",
    "    \n",
    "    #4. Extract first page to a DataFrame\n",
    "    df_t = page_table(soup, collection)\n",
    "    pag = 1\n",
    "    #   Print for visibility\n",
    "    #print(f\"First page was scraped from {collection}\")\n",
    "    \n",
    "    # 5. If there is more than 1 page \n",
    "    if pages > 1:\n",
    "        \n",
    "        #Print for visibility\n",
    "        print(\"Scraping more pages\")\n",
    "        \n",
    "        #5.1 Extract all pages and concatenate to the initial dataframe(pag.1)\n",
    "        \n",
    "        for i in range(pages+1):#index starts with 0\n",
    "            pag = int(i)+2 #first page was already sraped\n",
    "            if pag <= pages:\n",
    "                soup = acces_collections(collection,int(i)+2)\n",
    "                df_g = page_table(soup, collection)\n",
    "                df_t = pd.concat([df_t,df_g])\n",
    "    \n",
    "    print(f\"{pag-2} pages were scraped form {collection}\")\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb169e",
   "metadata": {},
   "source": [
    "### 7. Export the dataframe generated to a .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5459846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(data_frame, file_name):\n",
    "    \"\"\"\n",
    "    Export a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        - data_frame: pandas DataFrame\n",
    "        - file_name: str, name of the CSV file (without the extension)\n",
    "    \"\"\"\n",
    "    #print date stamp - datetime.datetime.now()\n",
    "    \n",
    "    t_stamp = str(datetime.datetime.now()).split(\".\")[0].replace(\" \",\"_\")\n",
    "    \n",
    "    file_path = f\"data/{file_name}_{t_stamp}.csv\"\n",
    "    \n",
    "    data_frame.to_csv(file_path, index=False)\n",
    "    print(f\"DataFrame successfully exported to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95a243",
   "metadata": {},
   "source": [
    "## SCRAPING DATA - MAIN FUNCTION\n",
    "\n",
    "We've analize the web to scrape and noticed that accesig collections by decades we can target all products in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accc2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the collections to srape:\n",
    "\n",
    "col = ['1960s','1970s','1980s','1990s','2000s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "bfe7f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scrapig 1960s collection ------\n",
      "<Response [200]>\n",
      "There is just 1 page\n",
      "-1 pages were scraped form 1960s\n",
      "DataFrame successfully exported to data/1960s_2024-02-04_01:22:42.csv\n",
      "------ Scrapig 1970s collection ------\n",
      "<Response [200]>\n",
      "Last page in pagination is nº12\n",
      "Scraping more pages\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "12 pages were scraped form 1970s\n",
      "DataFrame successfully exported to data/1970s_2024-02-04_01:23:00.csv\n",
      "------ Scrapig 1980s collection ------\n",
      "<Response [200]>\n",
      "Last page in pagination is nº36\n",
      "Scraping more pages\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "36 pages were scraped form 1980s\n",
      "DataFrame successfully exported to data/1980s_2024-02-04_01:24:17.csv\n",
      "------ Scrapig 1990s collection ------\n",
      "<Response [200]>\n",
      "Last page in pagination is nº48\n",
      "Scraping more pages\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "48 pages were scraped form 1990s\n",
      "DataFrame successfully exported to data/1990s_2024-02-04_01:25:59.csv\n",
      "------ Scrapig 2000s collection ------\n",
      "<Response [200]>\n",
      "Last page in pagination is nº5\n",
      "Scraping more pages\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "5 pages were scraped form 2000s\n",
      "DataFrame successfully exported to data/2000s_2024-02-04_01:26:10.csv\n"
     ]
    }
   ],
   "source": [
    "# main function:\n",
    "\n",
    "for i in col:\n",
    "    print(f'------ Scrapig {i} collection ------')\n",
    "    df = all_pages_collection(i)\n",
    "    export_to_csv(df, i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
