{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0929a388",
   "metadata": {},
   "source": [
    "# PRICING in VINTAGE SHIRTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5695cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#BeautifulSoup es una calse dentro de la libreria bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f2076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acces_collections(collection,page_no):\n",
    "    \n",
    "    \"This function generates a soup of the page 'x' of a filtered search based on collections\"\n",
    "    \n",
    "    #1. We defin the url for the search based on collection and page:\n",
    "    url = f\"https://www.wycovintage.com/collections/{collection}?filter.p.product_type=Shirt&options%5Bprefix%5D=last&page={page_no}&sort_by=created-descending\"\n",
    "\n",
    "    #2. Reques the html code for the url mentioned\n",
    "    html = requests.get(url)\n",
    "    \n",
    "    #3. Print the request resoponse, for visibility \n",
    "    print(html)\n",
    "    \n",
    "    #4. Return an objetct type BeautifulSoup with the content parsed\n",
    "    return BeautifulSoup(html.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894512ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_page(soup):\n",
    "    \n",
    "    \"This function returns the number of pages inside the same filtered shearch\"\n",
    "    \n",
    "    #1. Find the element where pagination/index is contained\n",
    "    index = soup.find(\"ul\", {\"class\":\"Pagination\"})\n",
    "    \n",
    "    #2. Extract all elements inside pagination/index that represet a number page - list returned\n",
    "    pags_list = index.find_all(\"a\", {\"class\":\"Button Pagination__Link\"})\n",
    "    \n",
    "    #3. Print the last page, for visibility \n",
    "    print(f'Last page in pagination is nÂº{pags_list[-1].getText()}')\n",
    "          \n",
    "    #3. We acces the last element of the list = last page and retunr the text\n",
    "    return pags_list[-1].getText()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dabed6",
   "metadata": {},
   "source": [
    "### Retrieve elements for a given page - debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953650a",
   "metadata": {},
   "source": [
    "Initial workarround:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ed35112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This was the original function proposed to retrieve the elements on each page\"\"\"\n",
    "\n",
    "def page_elements(soup):\n",
    "    \n",
    "    #1. Generate a list of descriptios for all the elements in the page:\n",
    "    desc = [i.getText().strip() for i in soup.find_all(\"h2\", {\"class\":\"ProductCard__Title\"})]\n",
    "    \n",
    "    #2. Generate a list of prices for all the elements in the page:\n",
    "    price = [i.getText().strip() for i in soup.find_all(\"span\", {\"class\":\"Price\"})]\n",
    "    \n",
    "    #Zipping desc and price\n",
    "    \n",
    "    #Tranforming to a dictionaty \n",
    "    \n",
    "    #Returning the dictionary\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46917df6",
   "metadata": {},
   "source": [
    "We have noticed that when generatin desc and price both list don't have the same length so we get\n",
    "an error while creating the dictionary as both list need to match in length\n",
    "\n",
    "To undertand what is happening we will tranform our original function into a debugging function \n",
    "to analice depper the posible causes of this bug:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b875c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"I tranformed this function into a debbug function to try to analice what is hapening\"\"\"\n",
    "\n",
    "def debugging(soup):\n",
    "    \n",
    "    #1. Generate a list of descriptios for all the elements in the page:\n",
    "    desc = [i.getText().strip() for i in soup.find_all(\"h2\", {\"class\":\"ProductCard__Title\"})]\n",
    "    \n",
    "    #2. Generate a list of prices for all the elements in the page:\n",
    "    price = [i.getText().strip() for i in soup.find_all(\"span\", {\"class\":\"Price\"})]\n",
    "    \n",
    "    print(f\"Descriptions list has {len(desc)} and the las one is {desc[-1]}\")\n",
    "    print(f\"Descriptions list has {len(price)} and the las one is {price[-1]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fcfd2",
   "metadata": {},
   "source": [
    "### Checkig for bugs: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8bac5",
   "metadata": {},
   "source": [
    "1. Inspectiong different pages in the same collection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b541441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 1979 Led Zeppelin Rules America Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1970s\",1)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970aef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 1970s Woodstock Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1970s\",2)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37752fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 1970s Dale Pulde War Eagle Orange County International Raceway Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1970s\",3)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c601e",
   "metadata": {},
   "source": [
    "1. Inspecting different collection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cdd7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 2 and the las one is 1968 Hardwick's Dinghy Derby Shirt\n",
      "Descriptions list has 3 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1960s\",1)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cde12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 1981 Siouxsie and the Banshees Ju-Ju Queen Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1980s\",3)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caef2cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 1993 Green Bay Packers Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"1990s\",7)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4482e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Descriptions list has 40 and the las one is 2000 Marilyn Manson Emperor Shirt\n",
      "Descriptions list has 41 and the las one is $68\n"
     ]
    }
   ],
   "source": [
    "soup = acces_collections(\"2000s\",4)\n",
    "debugging(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604c829",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "I noticed that price list allays have one more element and its always the same value: $68\n",
    "\n",
    "To avoid this bugg and future buggs wile matching the elements we wil target first the box where all info of a product is contain and then, will extratct the informations. This way we can ensure that we are getting the corresponging desc-item correcly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
